{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz 0\n",
    "\n",
    "### Submitted By: Stuti Sanghavi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Write a python program which reads the given file and outputs the frequency of all unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a fox jumped and jumped\n",
      "red fox jumped high\n",
      "a red high fox jumped and jumped\n",
      "red fox is red\n"
     ]
    }
   ],
   "source": [
    "# Reading text file\n",
    "file = open(\"foxy.txt\", \"r\")\n",
    "data = file.read()\n",
    "print (data)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting content of the file into individual words\n",
    "words = data.strip().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jumped    5\n",
       "red       4\n",
       "fox       4\n",
       "high      2\n",
       "a         2\n",
       "and       2\n",
       "is        1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting counts for unique words\n",
    "pd.Series(words).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 : What if your file has 100 billion records? how do you do that?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "If the file has 100 billion records, it will take a lot of time to run it on local machine. To better handle this, we can use HDFS with MapReduce and for example create 1000 clusters or nodes. Then we can provide input data, mapper function and reducer function. The following steps would then be executed:\n",
    "\n",
    "1. Input data will be partitioned and stored in different nodes\n",
    "2. Mapper function (split the string into individual words and create key-value pair i.e. {word, 1}) will be applied at each node\n",
    "3. The output of mapper function will be shuffled and sorted by the framework and transmitted to reducer nodes\n",
    "4. At the reducer nodes, reducer function (calculate sum of value grouped by key to provide unique word count i.e. {word, final_word_count}) will be applied to get the desired output\n",
    "5. Output from each of the reducer nodes will be collated and printed/stored in a file\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
