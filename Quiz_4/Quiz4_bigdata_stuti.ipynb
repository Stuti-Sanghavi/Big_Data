{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required packages\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lower, col\n",
    "from pyspark.sql import functions as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|hello|\n",
      "+-----+\n",
      "|spark|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Checking if spark session is working\n",
    "df = spark.sql('''select 'spark' as hello ''')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing raw data into a Dataframe\n",
    "df_whitehouse = spark.read.csv(\"whitehouse_waves-2016_12.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+-------+------+------+-----------+----+----+----+----+--------------+---------------+---------------+----------------+------------+--------------+----+---------------+---------------+----------------+-----------------+-----------+------------+----------------+-----------------+-----------+-----------+------------+\n",
      "|       NAMELAST|NAMEFIRST|NAMEMID|   UIN|BDGNBR|ACCESS_TYPE| TOA| POA| TOD| POD|APPT_MADE_DATE|APPT_START_DATE|  APPT_END_DATE|APPT_CANCEL_DATE|Total_People|LAST_UPDATEDBY|POST|  LASTENTRYDATE|TERMINAL_SUFFIX|visitee_namelast|visitee_namefirst|MEETING_LOC|MEETING_ROOM|CALLER_NAME_LAST|CALLER_NAME_FIRST|CALLER_ROOM|DESCRIPTION|Release_Date|\n",
      "+---------------+---------+-------+------+------+-----------+----+----+----+----+--------------+---------------+---------------+----------------+------------+--------------+----+---------------+---------------+----------------+-----------------+-----------+------------+----------------+-----------------+-----------+-----------+------------+\n",
      "|TAJOURIBESSASSI|   HANENE|   null|U22101|  null|         VA|null|null|null|null| 9/2/2015 0:00| 10/1/2015 3:00|10/1/2015 23:59|            null|           1|            AR| WIN| 9/2/2015 11:38|             AR|        Pelofsky|             Eric|       OEOB|         226|        ROWBERRY|           ARIANA|       null|       null|   1/29/2016|\n",
      "|        bageant|    laura|      j|U30528|  null|         VA|null|null|null|null|9/29/2015 0:00| 10/1/2015 5:00|9/30/2016 23:59|            null|           7|            WW| WIN|9/29/2015 13:42|             WW|     Baskerville|           Steven|         WH|  WH Grounds|          WARDEN|          WILLIAM|       null|       null|   1/29/2016|\n",
      "+---------------+---------+-------+------+------+-----------+----+----+----+----+--------------+---------------+---------------+----------------+------------+--------------+----+---------------+---------------+----------------+-----------------+-----------+------------+----------------+-----------------+-----------+-----------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_whitehouse.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+-----------------+----------------+\n",
      "|NAMEFIRST|       NAMELAST|visitee_namefirst|visitee_namelast|\n",
      "+---------+---------------+-----------------+----------------+\n",
      "|   HANENE|TAJOURIBESSASSI|             Eric|        Pelofsky|\n",
      "|    laura|        bageant|           Steven|     Baskerville|\n",
      "+---------+---------------+-----------------+----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Required Columns - NAMEFIRST, NAMELAST, visitee_namefirst, visitee_namelast\n",
    "df_req = df_whitehouse.select('NAMEFIRST', 'NAMELAST', 'visitee_namefirst', 'visitee_namelast')\n",
    "df_req.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the rows with null NAMELAST or visitee_namelast\n",
    "df_non_null = df_req.filter(df_req.NAMELAST.isNotNull() & df_req.visitee_namelast.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_null = df_req.filter(df_req.NAMELAST.isNull() | df_req.visitee_namelast.isNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59255"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_null.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NAMEFIRST', 'NAMELAST', 'visitee_namefirst', 'visitee_namelast']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_non_null.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lower = df_non_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+-----------------+----------------+\n",
      "|NAMEFIRST|       NAMELAST|visitee_namefirst|visitee_namelast|\n",
      "+---------+---------------+-----------------+----------------+\n",
      "|   hanene|tajouribessassi|             eric|        pelofsky|\n",
      "|    laura|        bageant|           steven|     baskerville|\n",
      "|     earl|       broemson|           steven|     baskerville|\n",
      "|  william|    jackling jr|           steven|     baskerville|\n",
      "|  richard|        mccrary|           steven|     baskerville|\n",
      "+---------+---------------+-----------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols = df_lower.columns\n",
    "\n",
    "#Creating a for loop to convert all the columns to lower case\n",
    "for col_name in cols:\n",
    "        df_lower = df_lower.withColumn(col_name, lower(col(col_name)))\n",
    "\n",
    "#Display data\n",
    "df_lower.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+-----------------+----------------+\n",
      "|NAMEFIRST|       NAMELAST|visitee_namefirst|visitee_namelast|\n",
      "+---------+---------------+-----------------+----------------+\n",
      "|   hanene|tajouribessassi|             eric|        pelofsky|\n",
      "|    laura|        bageant|           steven|     baskerville|\n",
      "|     earl|       broemson|           steven|     baskerville|\n",
      "|  william|    jackling jr|           steven|     baskerville|\n",
      "|  richard|        mccrary|           steven|     baskerville|\n",
      "+---------+---------------+-----------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Converting all data and column names to lower case \n",
    "\n",
    "#creating a new dataframe\n",
    "df_lower = df_non_null\n",
    "\n",
    "#Getting all the column names\n",
    "cols = df_lower.columns\n",
    "\n",
    "#Creating a for loop to convert all the columns to lower case\n",
    "for col_name in cols:\n",
    "        df_lower = df_lower.withColumn(col_name, lower(col(col_name)))\n",
    "\n",
    "#Display data\n",
    "df_lower.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_na = df_lower.na.drop(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "908699\n",
      "911249\n"
     ]
    }
   ],
   "source": [
    "####################### If a record is empty, then drop it\n",
    "print(df_lower.filter((df_lower.NAMEFIRST != \"\") & (df_lower.NAMELAST != \"\") & (df_lower.visitee_namefirst != \"\") & (df_lower.visitee_namelast != \"\")).count())\n",
    "print(df_lower.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "911249"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lower.filter(df_lower.visitee_namefirst.isNull()).count()\n",
    "df_lower.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'visitee_namelast'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-142-584be69599fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvisitee_namelast\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misNull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1302\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m             raise AttributeError(\n\u001b[1;32m-> 1304\u001b[1;33m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[0;32m   1305\u001b[0m         \u001b[0mjc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'visitee_namelast'"
     ]
    }
   ],
   "source": [
    "df.visitee_namelast.isNull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_lower\n",
    "n = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----+\n",
      "|NAMELAST|NAMEFIRST|count|\n",
      "+--------+---------+-----+\n",
      "| kidwell|   lauren|  222|\n",
      "|  thomas| benjamin|  196|\n",
      "|    haro|   steven|  183|\n",
      "|  berner|katherine|  177|\n",
      "|   grant|  patrick|  155|\n",
      "|    haas|   jordan|  152|\n",
      "|   garza|   steven|  127|\n",
      "|   cohen|    mandy|  122|\n",
      "|  martin|  kathryn|  122|\n",
      "|   brown| jennifer|  117|\n",
      "+--------+---------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#(a) The 10 most frequent visitors to the White House.\n",
    "#    (NAMELAST, NAMEFIRST)\n",
    "df.select('NAMELAST', 'NAMEFIRST').groupby('NAMELAST', 'NAMEFIRST').\\\n",
    "count().orderBy('count', ascending=False).show(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|          visitor|count|\n",
      "+-----------------+-----+\n",
      "|  kidwell, lauren|  222|\n",
      "| thomas, benjamin|  196|\n",
      "|     haro, steven|  183|\n",
      "|berner, katherine|  177|\n",
      "|   grant, patrick|  155|\n",
      "|     haas, jordan|  152|\n",
      "|    garza, steven|  127|\n",
      "|  martin, kathryn|  122|\n",
      "|     cohen, mandy|  122|\n",
      "|  brown, jennifer|  117|\n",
      "+-----------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('visitor', sf.concat(sf.col('NAMELAST'),sf.lit(', '), sf.col('NAMEFIRST')))\n",
    "\n",
    "df.select('visitor').groupby('visitor').\\\n",
    "count().orderBy('count', ascending=False).show(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`visitee_namelast, visitee_namefirst`' given input columns: [visitee, visitee_namefirst, NAMEFIRST, NAMELAST, visitor, visitee_namelast];;\\n'Project ['visitee_namelast, visitee_namefirst]\\n+- Project [NAMEFIRST#2013, NAMELAST#2018, visitee_namefirst#2023, visitee_namelast#2028, visitor#2110, concat(visitee_namelast#2028, , , visitee_namefirst#2023) AS visitee#2134]\\n   +- Project [NAMEFIRST#2013, NAMELAST#2018, visitee_namefirst#2023, visitee_namelast#2028, concat(NAMELAST#2018, , , NAMEFIRST#2013) AS visitor#2110]\\n      +- Project [NAMEFIRST#2013, NAMELAST#2018, visitee_namefirst#2023, lower(visitee_namelast#1831) AS visitee_namelast#2028]\\n         +- Project [NAMEFIRST#2013, NAMELAST#2018, lower(visitee_namefirst#1832) AS visitee_namefirst#2023, visitee_namelast#1831]\\n            +- Project [NAMEFIRST#2013, lower(NAMELAST#1812) AS NAMELAST#2018, visitee_namefirst#1832, visitee_namelast#1831]\\n               +- Project [lower(NAMEFIRST#1813) AS NAMEFIRST#2013, NAMELAST#1812, visitee_namefirst#1832, visitee_namelast#1831]\\n                  +- Filter (isnotnull(NAMELAST#1812) && isnotnull(visitee_namelast#1831))\\n                     +- Project [NAMEFIRST#1813, NAMELAST#1812, visitee_namefirst#1832, visitee_namelast#1831]\\n                        +- Relation[NAMELAST#1812,NAMEFIRST#1813,NAMEMID#1814,UIN#1815,BDGNBR#1816,ACCESS_TYPE#1817,TOA#1818,POA#1819,TOD#1820,POD#1821,APPT_MADE_DATE#1822,APPT_START_DATE#1823,APPT_END_DATE#1824,APPT_CANCEL_DATE#1825,Total_People#1826,LAST_UPDATEDBY#1827,POST#1828,LASTENTRYDATE#1829,TERMINAL_SUFFIX#1830,visitee_namelast#1831,visitee_namefirst#1832,MEETING_LOC#1833,MEETING_ROOM#1834,CALLER_NAME_LAST#1835,... 4 more fields] csv\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1294.select.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`visitee_namelast, visitee_namefirst`' given input columns: [visitee, visitee_namefirst, NAMEFIRST, NAMELAST, visitor, visitee_namelast];;\n'Project ['visitee_namelast, visitee_namefirst]\n+- Project [NAMEFIRST#2013, NAMELAST#2018, visitee_namefirst#2023, visitee_namelast#2028, visitor#2110, concat(visitee_namelast#2028, , , visitee_namefirst#2023) AS visitee#2134]\n   +- Project [NAMEFIRST#2013, NAMELAST#2018, visitee_namefirst#2023, visitee_namelast#2028, concat(NAMELAST#2018, , , NAMEFIRST#2013) AS visitor#2110]\n      +- Project [NAMEFIRST#2013, NAMELAST#2018, visitee_namefirst#2023, lower(visitee_namelast#1831) AS visitee_namelast#2028]\n         +- Project [NAMEFIRST#2013, NAMELAST#2018, lower(visitee_namefirst#1832) AS visitee_namefirst#2023, visitee_namelast#1831]\n            +- Project [NAMEFIRST#2013, lower(NAMELAST#1812) AS NAMELAST#2018, visitee_namefirst#1832, visitee_namelast#1831]\n               +- Project [lower(NAMEFIRST#1813) AS NAMEFIRST#2013, NAMELAST#1812, visitee_namefirst#1832, visitee_namelast#1831]\n                  +- Filter (isnotnull(NAMELAST#1812) && isnotnull(visitee_namelast#1831))\n                     +- Project [NAMEFIRST#1813, NAMELAST#1812, visitee_namefirst#1832, visitee_namelast#1831]\n                        +- Relation[NAMELAST#1812,NAMEFIRST#1813,NAMEMID#1814,UIN#1815,BDGNBR#1816,ACCESS_TYPE#1817,TOA#1818,POA#1819,TOD#1820,POD#1821,APPT_MADE_DATE#1822,APPT_START_DATE#1823,APPT_END_DATE#1824,APPT_CANCEL_DATE#1825,Total_People#1826,LAST_UPDATEDBY#1827,POST#1828,LASTENTRYDATE#1829,TERMINAL_SUFFIX#1830,visitee_namelast#1831,visitee_namefirst#1832,MEETING_LOC#1833,MEETING_ROOM#1834,CALLER_NAME_LAST#1835,... 4 more fields] csv\n\r\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:279)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3412)\r\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1340)\r\n\tat sun.reflect.GeneratedMethodAccessor78.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-148-0f7afa1c1491>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'visitee_namelast, visitee_namefirst'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'visitee_namelast, visitee_namefirst'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'count'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m   1322\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'Alice'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'Bob'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m         \"\"\"\n\u001b[1;32m-> 1324\u001b[1;33m         \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1325\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: \"cannot resolve '`visitee_namelast, visitee_namefirst`' given input columns: [visitee, visitee_namefirst, NAMEFIRST, NAMELAST, visitor, visitee_namelast];;\\n'Project ['visitee_namelast, visitee_namefirst]\\n+- Project [NAMEFIRST#2013, NAMELAST#2018, visitee_namefirst#2023, visitee_namelast#2028, visitor#2110, concat(visitee_namelast#2028, , , visitee_namefirst#2023) AS visitee#2134]\\n   +- Project [NAMEFIRST#2013, NAMELAST#2018, visitee_namefirst#2023, visitee_namelast#2028, concat(NAMELAST#2018, , , NAMEFIRST#2013) AS visitor#2110]\\n      +- Project [NAMEFIRST#2013, NAMELAST#2018, visitee_namefirst#2023, lower(visitee_namelast#1831) AS visitee_namelast#2028]\\n         +- Project [NAMEFIRST#2013, NAMELAST#2018, lower(visitee_namefirst#1832) AS visitee_namefirst#2023, visitee_namelast#1831]\\n            +- Project [NAMEFIRST#2013, lower(NAMELAST#1812) AS NAMELAST#2018, visitee_namefirst#1832, visitee_namelast#1831]\\n               +- Project [lower(NAMEFIRST#1813) AS NAMEFIRST#2013, NAMELAST#1812, visitee_namefirst#1832, visitee_namelast#1831]\\n                  +- Filter (isnotnull(NAMELAST#1812) && isnotnull(visitee_namelast#1831))\\n                     +- Project [NAMEFIRST#1813, NAMELAST#1812, visitee_namefirst#1832, visitee_namelast#1831]\\n                        +- Relation[NAMELAST#1812,NAMEFIRST#1813,NAMEMID#1814,UIN#1815,BDGNBR#1816,ACCESS_TYPE#1817,TOA#1818,POA#1819,TOD#1820,POD#1821,APPT_MADE_DATE#1822,APPT_START_DATE#1823,APPT_END_DATE#1824,APPT_CANCEL_DATE#1825,Total_People#1826,LAST_UPDATEDBY#1827,POST#1828,LASTENTRYDATE#1829,TERMINAL_SUFFIX#1830,visitee_namelast#1831,visitee_namefirst#1832,MEETING_LOC#1833,MEETING_ROOM#1834,CALLER_NAME_LAST#1835,... 4 more fields] csv\\n\""
     ]
    }
   ],
   "source": [
    "df.select('visitee_namelast', 'visitee_namefirst').groupby('visitee_namelast', 'visitee_namefirst').\\\n",
    "count().orderBy('count', ascending=False).show(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|             visitee| count|\n",
      "+--------------------+------+\n",
      "|    office, visitors|430881|\n",
      "|waves, visitorsof...| 44129|\n",
      "|        bryant, ruth| 13970|\n",
      "|       oneil, olivia| 13155|\n",
      "|     thompson, jared| 11618|\n",
      "|            /, potus| 10900|\n",
      "|      burton, collin|  9672|\n",
      "|      megan, matthew|  7944|\n",
      "|     mayerson, asher|  6886|\n",
      "| dessources, kalisha|  5289|\n",
      "+--------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#(b) The 10 most frequently visited people in \n",
    "#    the White House.\n",
    "#    (visitee_namelast, visitee_namefirst) \n",
    "\n",
    "df = df.na.fill({'visitee_namefirst': '', 'NAMEFIRST': ''})\n",
    "\n",
    "df = df.withColumn('visitee', sf.concat(sf.col('visitee_namelast'),sf.lit(', '), sf.col('visitee_namefirst')))\n",
    "\n",
    "\n",
    "df.select('visitee').groupby('visitee').\\\n",
    "count().orderBy('count', ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+----------------+-----------------+-----+\n",
      "|NAMELAST|NAMEFIRST|visitee_namelast|visitee_namefirst|count|\n",
      "+--------+---------+----------------+-----------------+-----+\n",
      "| kidwell|   lauren|        yudelson|             alex|  103|\n",
      "|    haas|   jordan|        yudelson|             alex|   90|\n",
      "|   grant|  patrick|        yudelson|             alex|   89|\n",
      "|  thomas| benjamin|        yudelson|             alex|   89|\n",
      "|    haro|   steven|        yudelson|             alex|   84|\n",
      "|   cohen|    mandy|         lambrew|           jeanne|   84|\n",
      "|  berner|katherine|        yudelson|             alex|   82|\n",
      "|   roche|  shannon|        yudelson|             alex|   70|\n",
      "|  urizar| jennifer|         johnson|            katie|   68|\n",
      "|  martin|  kathryn|         lambrew|           jeanne|   61|\n",
      "+--------+---------+----------------+-----------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#(c) The 10 most frequent visitor-visitee  combinations.\n",
    "df.groupby('NAMELAST', 'NAMEFIRST','visitee_namelast', 'visitee_namefirst').\\\n",
    "count().orderBy('count', ascending=False).show(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|     visitor-visitee|count|\n",
      "+--------------------+-----+\n",
      "|kidwell, lauren -...|  103|\n",
      "|haas, jordan - yu...|   90|\n",
      "|grant, patrick - ...|   89|\n",
      "|thomas, benjamin ...|   89|\n",
      "|cohen, mandy - la...|   84|\n",
      "|haro, steven - yu...|   84|\n",
      "|berner, katherine...|   82|\n",
      "|roche, shannon - ...|   70|\n",
      "|urizar, jennifer ...|   68|\n",
      "|martin, kathryn -...|   61|\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('visitor-visitee', sf.concat(sf.col('visitor'),sf.lit(' - '), sf.col('visitee')))\n",
    "\n",
    "df.groupby('visitor-visitee').\\\n",
    "count().orderBy('count', ascending=False).show(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------------+----------------+-----------------+-------+---------------+\n",
      "|NAMEFIRST| NAMELAST|visitee_namefirst|visitee_namelast|          visitor|visitee|visitor-visitee|\n",
      "+---------+---------+-----------------+----------------+-----------------+-------+---------------+\n",
      "|    tanya|    matos|             null|         yushika|     matos, tanya|   null|           null|\n",
      "|     ryan| mcdonald|             null|         yushika|   mcdonald, ryan|   null|           null|\n",
      "|   steven|   mufson|             null|         yushika|   mufson, steven|   null|           null|\n",
      "|   thomas|  prudden|             null|          marsha|  prudden, thomas|   null|           null|\n",
      "|    bryan| strother|             null|          marsha|  strother, bryan|   null|           null|\n",
      "|   robert|   custer|             null|          marsha|   custer, robert|   null|           null|\n",
      "|    peter|    doocy|             null|         yushika|     doocy, peter|   null|           null|\n",
      "|   andrew|rompalski|             null|          marsha|rompalski, andrew|   null|           null|\n",
      "|      ali| weinberg|             null|         deborah|    weinberg, ali|   null|           null|\n",
      "|   stefan|comparini|             null|         yushika|comparini, stefan|   null|           null|\n",
      "|  brandon|  mahoney|             null|         yushika| mahoney, brandon|   null|           null|\n",
      "|    kevin|   popham|             null|         yushika|    popham, kevin|   null|           null|\n",
      "|    david|strogylos|             null|         yushika| strogylos, david|   null|           null|\n",
      "|   robert|   riedel|             null|          marsha|   riedel, robert|   null|           null|\n",
      "|   denise|    lynch|             null|         raymond|    lynch, denise|   null|           null|\n",
      "|     eric|    huynh|             null|         deborah|      huynh, eric|   null|           null|\n",
      "|  richard|    rabin|             null|          marsha|   rabin, richard|   null|           null|\n",
      "|    peter|    kolby|             null|          marsha|     kolby, peter|   null|           null|\n",
      "|   carlos| reymundi|             null|         deborah| reymundi, carlos|   null|           null|\n",
      "|    diana| castanda|             null|          marsha|  castanda, diana|   null|           null|\n",
      "+---------+---------+-----------------+----------------+-----------------+-------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.visitee.isNull()).show()\n",
    "#df_req.NAMELAST.isNotNull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------------+-----+\n",
      "|          visitor|        visitee|count|\n",
      "+-----------------+---------------+-----+\n",
      "|  kidwell, lauren| yudelson, alex|  103|\n",
      "|     haas, jordan| yudelson, alex|   90|\n",
      "|   grant, patrick| yudelson, alex|   89|\n",
      "| thomas, benjamin| yudelson, alex|   89|\n",
      "|     cohen, mandy|lambrew, jeanne|   84|\n",
      "|     haro, steven| yudelson, alex|   84|\n",
      "|berner, katherine| yudelson, alex|   82|\n",
      "|   roche, shannon| yudelson, alex|   70|\n",
      "| urizar, jennifer| johnson, katie|   68|\n",
      "|  martin, kathryn|lambrew, jeanne|   61|\n",
      "+-----------------+---------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('visitor','visitee').\\\n",
    "count().orderBy('count', ascending=False).show(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records dropped = 59255\n",
      "Number of records processed = 911249\n"
     ]
    }
   ],
   "source": [
    "initial_records = df_req.count()\n",
    "final_records = df.count()\n",
    "\n",
    "print('Number of records dropped = ' + str(initial_records-final_records))\n",
    "print('Number of records processed = ' + str(final_records))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
